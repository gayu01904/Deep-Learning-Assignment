NAME	  Gayathri G
USN	    1CD22AI020
SUBJECT	Deep Learning and Reinforcement Learning
SUBJECT CODE	BAI701

# Deep-Learning-Assignment
üìå Assignment Overview
This repository contains optimized and improved implementations of various Deep Learning and Reinforcement Learning models.
The original programs were modified to improve performance, learning stability, practical usability, and clarity of outputs and limited hardware environments.
All modifications were done with a focus on:
‚Ä¢	Reducing computational complexity
‚Ä¢	Improving convergence and learning behavior
‚Ä¢	Producing clearer and more meaningful results
________________________________________
üéØ Objectives of the Modifications
‚Ä¢	Optimize model architectures for faster execution
‚Ä¢	Improve learning stability and convergence
‚Ä¢	Enhance output quality (text generation, gameplay, forecasting)
________________________________________
üìÇ File-Wise Description of Changes
1Ô∏è‚É£ AlexNet.py
Original Version:
A heavy AlexNet architecture with large convolution filters and very large fully connected layers, mainly suitable for large-scale datasets.
Modification Purpose:
To reduce model size and computational load while maintaining learning capability.
Effectiveness:
‚Ä¢	Faster training and execution
‚Ä¢	Reduced memory usage
‚Ä¢	More stable training using a custom learning rate
‚úÖ Result: Optimized and train-ready CNN suitable for academic experiments.
________________________________________

2Ô∏è‚É£ TicTacToe.py
Original Version:
A basic reinforcement learning implementation with weak rewards, high randomness, and minor initialization issues.
Modification Purpose:
To improve AI decision-making, learning speed, and gameplay stability.
Effectiveness:
‚Ä¢	Faster convergence
‚Ä¢	Smarter and more strategic gameplay
‚Ä¢	Stable execution without runtime errors
‚úÖ Result: Clearly improved reinforcement learning behavior.
________________________________________
3Ô∏è‚É£ RNN.py
Original Version:
A simple RNN using short sequences, ReLU activation, and deterministic text generation, resulting in repetitive output.
Modification Purpose:
To enhance sequence learning, stability, and creativity in generated text.
Effectiveness:
‚Ä¢	More natural and less repetitive text
‚Ä¢	Creative text generation using temperature sampling
‚Ä¢	Improved training stability with tanh activation
‚úÖ Result: Higher-quality text generation.
________________________________________
4Ô∏è‚É£ DeepReinforcementLearning.py
Original Version:
A basic Q-learning model with sparse rewards and unstable learning patterns.
Modification Purpose:
To improve learning efficiency, convergence stability, and path optimality.
Effectiveness:
‚Ä¢	Faster goal achievement
‚Ä¢	Smoother learning curve
‚Ä¢	Consistent and optimal paths
‚úÖ Result: Stable and interpretable Q-learning model.
________________________________________
5Ô∏è‚É£ LSTM.py
Original Version:
A single-step LSTM model predicting only the next time-step value.
Modification Purpose:
To enable multi-step future prediction for realistic time-series forecasting.
Effectiveness:
‚Ä¢	Multi-step forecasting capability
‚Ä¢	More useful and practical predictions
‚Ä¢	Improved training stability
‚úÖ Result: Advanced and real-world applicable forecasting model.
________________________________________
üß™ Technologies Used
‚Ä¢	Python
‚Ä¢	TensorFlow / Keras
‚Ä¢	NumPy
‚Ä¢	Reinforcement Learning Concepts
‚Ä¢	Deep Learning Architectures (CNN, RNN, LSTM)
________________________________________

